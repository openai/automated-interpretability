{
    "summary": "This code includes a `PromptFormat` class for formatting methods, a `HarmonyMessage` dictionary for roles and content, and a `PromptBuilder` class to create prompts with token counting using GPT-4 encoding. It checks roles, creates deep copies of messages, and handles system messages. The code also checks the last user message and appends \"<|endofprompt|>\" before returning either a list of messages' contents or concatenating them into a single string, while raising a ValueError for unknown prompt formats.",
    "details": [
        {
            "comment": "The code defines a class `PromptFormat` which is an enumeration of different prompt formatting methods. The `HarmonyMessage` is a typed dictionary defining the role and content of each message in the prompt. There's also a method `from_string` that returns the corresponding `PromptFormat` from a string input.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/explanations/prompt_builder.py\":0-37",
            "content": "from __future__ import annotations\nfrom enum import Enum\nfrom typing import TypedDict, Union\nimport tiktoken\nHarmonyMessage = TypedDict(\n    \"HarmonyMessage\",\n    {\n        \"role\": str,\n        \"content\": str,\n    },\n)\nclass PromptFormat(str, Enum):\n    \"\"\"\n    Different ways of formatting the components of a prompt into the format accepted by the relevant\n    API server endpoint.\n    \"\"\"\n    NONE = \"none\"\n    \"\"\"Suitable for use with models that don't use special tokens for instructions.\"\"\"\n    INSTRUCTION_FOLLOWING = \"instruction_following\"\n    \"\"\"Suitable for IF models that use <|endofprompt|>.\"\"\"\n    HARMONY_V4 = \"harmony_v4\"\n    \"\"\"\n    Suitable for Harmony models that use a structured turn-taking role+content format. Generates a\n    list of HarmonyMessage dicts that can be sent to the /chat/completions endpoint.\n    \"\"\"\n    @classmethod\n    def from_string(cls, s: str) -> PromptFormat:\n        for prompt_format in cls:\n            if prompt_format.value == s:\n                return prompt_format\n        raise ValueError(f\"{s} is not a valid PromptFormat\")"
        },
        {
            "comment": "This code defines a PromptBuilder class for creating prompts. It initializes an empty list of HarmonyMessages and has methods to add messages and calculate the prompt's length in tokens using GPT-4 encoding.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/explanations/prompt_builder.py\":40-65",
            "content": "class Role(str, Enum):\n    \"\"\"See https://platform.openai.com/docs/guides/chat\"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\nclass PromptBuilder:\n    \"\"\"Class for accumulating components of a prompt and then formatting them into an output.\"\"\"\n    def __init__(self) -> None:\n        self._messages: list[HarmonyMessage] = []\n    def add_message(self, role: Role, message: str) -> None:\n        self._messages.append(HarmonyMessage(role=role, content=message))\n    def prompt_length_in_tokens(self, prompt_format: PromptFormat) -> int:\n        # TODO(sbills): Make the model/encoding configurable. This implementation assumes GPT-4.\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        if prompt_format == PromptFormat.HARMONY_V4:\n            # Approximately-correct implementation adapted from this documentation:\n            # https://platform.openai.com/docs/guides/chat/introduction\n            num_tokens = 0\n            for message in self._messages:\n                num_tokens += ("
        },
        {
            "comment": "This code calculates the number of tokens in a given message by encoding it and adding the length to a running total. If no specific format is provided, it builds a prompt according to a specified format and encodes it for token counting. The build function also validates the alternation of assistant and user messages.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/explanations/prompt_builder.py\":66-84",
            "content": "                    4  # every message follows <|im_start|>{role/name}\\n{content}<|im_end|>\\n\n                )\n                num_tokens += len(encoding.encode(message[\"content\"], allowed_special=\"all\"))\n            num_tokens += 2  # every reply is primed with <|im_start|>assistant\n            return num_tokens\n        else:\n            prompt_str = self.build(prompt_format)\n            assert isinstance(prompt_str, str)\n            return len(encoding.encode(prompt_str, allowed_special=\"all\"))\n    def build(\n        self, prompt_format: PromptFormat, *, allow_extra_system_messages: bool = False\n    ) -> Union[str, list[HarmonyMessage]]:\n        \"\"\"\n        Validates the messages added so far (reasonable alternation of assistant vs. user, etc.)\n        and returns either a regular string (maybe with <|endofprompt|> tokens) or a list of\n        HarmonyMessages suitable for use with the /chat/completions endpoint.\n        The `allow_extra_system_messages` parameter allows the caller to specify that the prompt"
        },
        {
            "comment": "The code creates a deep copy of the messages to prevent any external modification. It then checks if the next message is from the expected role and allows extra system messages if specified. Finally, it prepares for prompt formatting if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/explanations/prompt_builder.py\":85-107",
            "content": "        should be allowed to contain system messages after the very first one.\n        \"\"\"\n        # Create a deep copy of the messages so we can modify it and so that the caller can't\n        # modify the internal state of this object.\n        messages = [message.copy() for message in self._messages]\n        expected_next_role = Role.SYSTEM\n        for message in messages:\n            role = message[\"role\"]\n            assert role == expected_next_role or (\n                allow_extra_system_messages and role == Role.SYSTEM\n            ), f\"Expected message from {expected_next_role} but got message from {role}\"\n            if role == Role.SYSTEM:\n                expected_next_role = Role.USER\n            elif role == Role.USER:\n                expected_next_role = Role.ASSISTANT\n            elif role == Role.ASSISTANT:\n                expected_next_role = Role.USER\n        if prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:\n            last_user_message = None\n            for message in messages:\n                if message[\"role\"] == Role.USER:"
        },
        {
            "comment": "This code checks the last user message and appends \"<|endofprompt|>\" to its content. Depending on the prompt format, it either returns a list of messages' contents or concatenates them into a single string. If an unknown prompt format is encountered, it raises a ValueError.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/explanations/prompt_builder.py\":108-117",
            "content": "                    last_user_message = message\n            assert last_user_message is not None\n            last_user_message[\"content\"] += \"<|endofprompt|>\"\n        if prompt_format == PromptFormat.HARMONY_V4:\n            return messages\n        elif prompt_format in [PromptFormat.NONE, PromptFormat.INSTRUCTION_FOLLOWING]:\n            return \"\".join(message[\"content\"] for message in messages)\n        else:\n            raise ValueError(f\"Unknown prompt format: {prompt_format}\")"
        }
    ]
}