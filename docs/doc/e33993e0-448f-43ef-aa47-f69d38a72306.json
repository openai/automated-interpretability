{
    "summary": "The code handles activation records, features for max values and formatting neuron activations, and marks activations as unknown based on user inputs. It also calculates the ratio of non-zero activations to total activations across all records.",
    "details": [
        {
            "comment": "This code defines functions to handle activation records, including calculating the maximum activation value and normalizing neuron activations. It also includes a REALU function for handling activation values less than 0 as resting state indicators.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/activations/activation_records.py\":0-28",
            "content": "\"\"\"Utilities for formatting activation records into prompts.\"\"\"\nimport math\nfrom typing import Optional, Sequence\nfrom neuron_explainer.activations.activations import ActivationRecord\nUNKNOWN_ACTIVATION_STRING = \"unknown\"\ndef relu(x: float) -> float:\n    return max(0.0, x)\ndef calculate_max_activation(activation_records: Sequence[ActivationRecord]) -> float:\n    \"\"\"Return the maximum activation value of the neuron across all the activation records.\"\"\"\n    flattened = [\n        # Relu is used to assume any values less than 0 are indicating the neuron is in the resting\n        # state. This is a simplifying assumption that works with relu/gelu.\n        max(relu(x) for x in activation_record.activations)\n        for activation_record in activation_records\n    ]\n    return max(flattened)\ndef normalize_activations(activation_record: list[float], max_activation: float) -> list[int]:\n    \"\"\"Convert raw neuron activations to integers on the range [0, 10].\"\"\"\n    if max_activation <= 0:\n        return [0 for x in activation_record]"
        },
        {
            "comment": "The code snippet is responsible for formatting neuron activations into a string. It first applies an optional normalization to the activations, then optionally removes zeros and hides activations based on user inputs. The resulting string contains tokens and their corresponding normalized or hidden activations, suitable for use in prompts.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/activations/activation_records.py\":29-52",
            "content": "    # Relu is used to assume any values less than 0 are indicating the neuron is in the resting\n    # state. This is a simplifying assumption that works with relu/gelu.\n    return [min(10, math.floor(10 * relu(x) / max_activation)) for x in activation_record]\ndef _format_activation_record(\n    activation_record: ActivationRecord,\n    max_activation: float,\n    omit_zeros: bool,\n    hide_activations: bool = False,\n    start_index: int = 0,\n) -> str:\n    \"\"\"Format neuron activations into a string, suitable for use in prompts.\"\"\"\n    tokens = activation_record.tokens\n    normalized_activations = normalize_activations(activation_record.activations, max_activation)\n    if omit_zeros:\n        assert (not hide_activations) and start_index == 0, \"Can't hide activations and omit zeros\"\n        tokens = [\n            token for token, activation in zip(tokens, normalized_activations) if activation > 0\n        ]\n        normalized_activations = [x for x in normalized_activations if x > 0]\n    entries = []\n    assert len(tokens) == len(normalized_activations)"
        },
        {
            "comment": "The code formats a list of activation records into a string. It iterates through each token, activation pair and normalizes the activations. If hide_activations or index is less than start_index, it replaces activation with UNKNOWN_ACTIVATION_STRING. The final formatted string joins entries with newline characters and includes <start> and <end> markers.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/activations/activation_records.py\":53-80",
            "content": "    for index, token, activation in zip(range(len(tokens)), tokens, normalized_activations):\n        activation_string = str(int(activation))\n        if hide_activations or index < start_index:\n            activation_string = UNKNOWN_ACTIVATION_STRING\n        entries.append(f\"{token}\\t{activation_string}\")\n    return \"\\n\".join(entries)\ndef format_activation_records(\n    activation_records: Sequence[ActivationRecord],\n    max_activation: float,\n    *,\n    omit_zeros: bool = False,\n    start_indices: Optional[list[int]] = None,\n    hide_activations: bool = False,\n) -> str:\n    \"\"\"Format a list of activation records into a string.\"\"\"\n    return (\n        \"\\n<start>\\n\"\n        + \"\\n<end>\\n<start>\\n\".join(\n            [\n                _format_activation_record(\n                    activation_record,\n                    max_activation,\n                    omit_zeros=omit_zeros,\n                    hide_activations=hide_activations,\n                    start_index=0 if start_indices is None else start_indices[i],\n                )"
        },
        {
            "comment": "This code contains several functions to format and manipulate activation records and tokens for simulation purposes. The `_format_tokens_for_simulation` function formats a sequence of strings into a string with each token marked as having an \"unknown\" activation, suitable for use in prompts. The `format_sequences_for_simulation` function extends this to format a list of lists of tokens into a string with each token marked as having an \"unknown\" activation, also suitable for use in prompts. Finally, the `non_zero_activation_proportion` function calculates the proportion of non-zero activation values among a sequence of ActivationRecord objects.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/activations/activation_records.py\":81-118",
            "content": "                for i, activation_record in enumerate(activation_records)\n            ]\n        )\n        + \"\\n<end>\\n\"\n    )\ndef _format_tokens_for_simulation(tokens: Sequence[str]) -> str:\n    \"\"\"\n    Format tokens into a string with each token marked as having an \"unknown\" activation, suitable\n    for use in prompts.\n    \"\"\"\n    entries = []\n    for token in tokens:\n        entries.append(f\"{token}\\t{UNKNOWN_ACTIVATION_STRING}\")\n    return \"\\n\".join(entries)\ndef format_sequences_for_simulation(\n    all_tokens: Sequence[Sequence[str]],\n) -> str:\n    \"\"\"\n    Format a list of lists of tokens into a string with each token marked as having an \"unknown\"\n    activation, suitable for use in prompts.\n    \"\"\"\n    return (\n        \"\\n<start>\\n\"\n        + \"\\n<end>\\n<start>\\n\".join(\n            [_format_tokens_for_simulation(tokens) for tokens in all_tokens]\n        )\n        + \"\\n<end>\\n\"\n    )\ndef non_zero_activation_proportion(\n    activation_records: Sequence[ActivationRecord], max_activation: float\n) -> float:\n    \"\"\"Return the proportion of activation values that aren't zero.\"\"\""
        },
        {
            "comment": "Calculating the ratio of non-zero activations to total activations across all activation records.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/activations/activation_records.py\":119-129",
            "content": "    total_activations_count = sum(\n        [len(activation_record.activations) for activation_record in activation_records]\n    )\n    normalized_activations = [\n        normalize_activations(activation_record.activations, max_activation)\n        for activation_record in activation_records\n    ]\n    non_zero_activations_count = sum(\n        [len([x for x in activations if x != 0]) for activations in normalized_activations]\n    )\n    return non_zero_activations_count / total_activations_count"
        }
    ]
}