{
    "summary": "The code initializes an API client with error handling, response caching for OpenAI requests, and implements exponential backoff for retry mechanisms. It uses HTTPX to make asynchronous requests and starts an event loop in the main function.",
    "details": [
        {
            "comment": "This function checks if the error thrown is an API error or a connection error. If it's an API error with status code 400, 404, or 415, it may be due to an idempotency error and can be retried. Otherwise, if it's a connection error, it also needs to be retried.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/api_client.py\":0-33",
            "content": "import asyncio\nimport contextlib\nimport os\nimport random\nimport traceback\nfrom asyncio import Semaphore\nfrom functools import wraps\nfrom typing import Any, Callable, Optional\nimport httpx\nimport orjson\ndef is_api_error(err: Exception) -> bool:\n    if isinstance(err, httpx.HTTPStatusError):\n        response = err.response\n        error_data = response.json().get(\"error\", {})\n        error_message = error_data.get(\"message\")\n        if response.status_code in [400, 404, 415]:\n            if error_data.get(\"type\") == \"idempotency_error\":\n                print(f\"Retrying after idempotency error: {error_message} ({response.url})\")\n                return True\n            else:\n                # Invalid request\n                return False\n        else:\n            print(f\"Retrying after API error: {error_message} ({response.url})\")\n            return True\n    elif isinstance(err, httpx.ConnectError):\n        print(f\"Retrying after connection error... ({err.request.url})\")\n        return True\n    elif isinstance(err, httpx.TimeoutException):"
        },
        {
            "comment": "The code defines a decorator function `exponential_backoff` that retries a wrapped asynchronous function with exponential backoff and jitter after failures. The retry attempts continue until the specified `retry_on` condition returns False or the maximum number of tries is reached. It also prints error messages and stack traces for unexpected errors during the retries.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/api_client.py\":34-65",
            "content": "        print(f\"Retrying after a timeout error... ({err.request.url})\")\n        return True\n    elif isinstance(err, httpx.ReadError):\n        print(f\"Retrying after a read error... ({err.request.url})\")\n        return True\n    print(f\"Retrying after an unexpected error: {repr(err)}\")\n    traceback.print_tb(err.__traceback__)\n    return True\ndef exponential_backoff(\n    retry_on: Callable[[Exception], bool] = lambda err: True\n) -> Callable[[Callable], Callable]:\n    \"\"\"\n    Returns a decorator which retries the wrapped function as long as the specified retry_on\n    function returns True for the exception, applying exponential backoff with jitter after\n    failures, up to a retry limit.\n    \"\"\"\n    init_delay_s = 1.0\n    max_delay_s = 10.0\n    # Roughly 30 minutes before we give up.\n    max_tries = 200\n    backoff_multiplier = 2.0\n    jitter = 0.2\n    def decorate(f: Callable) -> Callable:\n        assert asyncio.iscoroutinefunction(f)\n        @wraps(f)\n        async def f_retry(*args: Any, **kwargs: Any) -> None:"
        },
        {
            "comment": "Code snippet handles API requests with retry mechanism and error handling. It sets the OpenAI API key, HTTP headers, and base API URL for making requests. The ApiClient class is initialized with a model_name parameter and supports response caching and concurrency limits.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/api_client.py\":66-97",
            "content": "            delay_s = init_delay_s\n            for i in range(max_tries):\n                try:\n                    return await f(*args, **kwargs)\n                except Exception as err:\n                    if not retry_on(err) or i == max_tries - 1:\n                        raise\n                    jittered_delay = random.uniform(delay_s * (1 - jitter), delay_s * (1 + jitter))\n                    await asyncio.sleep(jittered_delay)\n                    delay_s = min(delay_s * backoff_multiplier, max_delay_s)\n        return f_retry\n    return decorate\nAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\nassert API_KEY, \"Please set the OPENAI_API_KEY environment variable\"\nAPI_HTTP_HEADERS = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + API_KEY,\n}\nBASE_API_URL = \"https://api.openai.com/v1\"\nclass ApiClient:\n    \"\"\"Performs inference using the OpenAI API. Supports response caching and concurrency limits.\"\"\"\n    def __init__(\n        self,\n        model_name: str,\n        # If set, no more than this number of HTTP requests will be made concurrently."
        },
        {
            "comment": "The code is initializing an API client with optional parameters for maximum concurrent requests, and whether to cache request/response pairs. It also has a method `make_request` which uses exponential backoff retry mechanism when making HTTP calls. If caching is enabled, it checks if the request has been cached before executing the call.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/api_client.py\":98-125",
            "content": "        max_concurrent: Optional[int] = None,\n        # Whether to cache request/response pairs in memory to avoid duplicating requests.\n        cache: bool = False,\n    ):\n        self.model_name = model_name\n        if max_concurrent is not None:\n            self._concurrency_check: Optional[Semaphore] = Semaphore(max_concurrent)\n        else:\n            self._concurrency_check = None\n        if cache:\n            self._cache: Optional[dict[str, Any]] = {}\n        else:\n            self._cache = None\n    @exponential_backoff(retry_on=is_api_error)\n    async def make_request(\n        self, timeout_seconds: Optional[int] = None, **kwargs: Any\n    ) -> dict[str, Any]:\n        if self._cache is not None:\n            key = orjson.dumps(kwargs)\n            if key in self._cache:\n                return self._cache[key]\n        async with contextlib.AsyncExitStack() as stack:\n            if self._concurrency_check is not None:\n                await stack.enter_async_context(self._concurrency_check)\n            http_client = await stack.enter_async_context("
        },
        {
            "comment": "This code is creating an instance of `ApiClient` class and making a request to OpenAI API using the `make_request` method. The request URL depends on whether the input has \"messages\" key or not, and it uses HTTPX client for asynchronous requests. If there's an error in the response, it prints the JSON data then re-raises the exception. If a cache is set, the response JSON will be cached under the specified key.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/api_client.py\":126-149",
            "content": "                httpx.AsyncClient(timeout=timeout_seconds)\n            )\n            # If the request has a \"messages\" key, it should be sent to the /chat/completions\n            # endpoint. Otherwise, it should be sent to the /completions endpoint.\n            url = BASE_API_URL + (\"/chat/completions\" if \"messages\" in kwargs else \"/completions\")\n            kwargs[\"model\"] = self.model_name\n            response = await http_client.post(url, headers=API_HTTP_HEADERS, json=kwargs)\n        # The response json has useful information but the exception doesn't include it, so print it\n        # out then reraise.\n        try:\n            response.raise_for_status()\n        except Exception as e:\n            print(response.json())\n            raise e\n        if self._cache is not None:\n            self._cache[key] = response.json()\n        return response.json()\nif __name__ == \"__main__\":\n    async def main() -> None:\n        client = ApiClient(model_name=\"gpt-3.5-turbo\", max_concurrent=1)\n        print(await client.make_request(prompt=\"Why did the chicken cross the road?\", max_tokens=9))"
        },
        {
            "comment": "This code starts an asynchronous event loop and runs the main function.",
            "location": "\"/media/root/Toshiba XG3/works/automated-interpretability/docs/src/neuron-explainer/neuron_explainer/api_client.py\":151-151",
            "content": "    asyncio.run(main())"
        }
    ]
}